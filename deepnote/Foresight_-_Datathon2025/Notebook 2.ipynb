{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "d76ac53a",
        "execution_start": 1763245822826,
        "execution_millis": 3035579,
        "execution_context_id": "e448880f-9f97-446f-944a-f74151a5847a",
        "cell_id": "2a8f18b66a9542ffbf8dd3f249f38391",
        "deepnote_cell_type": "code"
      },
      "source": "# Refactored end-to-end pipeline for Risk-Drift + Adversarial Fragility\n# Save as pipeline_refactor.py or run inside a Jupyter notebook cell.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# --- CONFIG --- #\nDATA_PATH = \"Health_Risk_Dataset.csv\"\nFEATURES = ['Respiratory_Rate','Oxygen_Saturation','O2_Scale','Systolic_BP',\n            'Heart_Rate','Temperature','Consciousness','On_Oxygen']\nCONTINUOUS_VITALS = ['Respiratory_Rate','Oxygen_Saturation','Systolic_BP','Heart_Rate','Temperature']\n\nSAFE_RANGES = {\n    'Respiratory_Rate': (12, 20),\n    'Oxygen_Saturation': (94, 100),\n    'Systolic_BP': (100, 140),\n    'Heart_Rate': (60, 100),\n    'Temperature': (36.0, 37.5)\n}\n\nCONSCIOUSNESS_MAP = {'A':0,'C':1,'P':2,'V':3,'U':4}\nRISK_MAP = {'Normal':0,'Low':1,'Medium':2,'High':3}\nINV_RISK_MAP = {v:k for k,v in RISK_MAP.items()}\n\n# --- Load --- #\nif not Path(DATA_PATH).exists():\n    raise FileNotFoundError(f\"{DATA_PATH} not found. Place the CSV in working directory.\")\ndf = pd.read_csv(DATA_PATH, sep=None, engine=\"python\")\ndf.columns = [c.strip() for c in df.columns]\n\n# quick schema check\nexpected = {'Patient_ID','Risk_Level'}\nif not expected.issubset(set(df.columns)):\n    raise ValueError(f\"CSV missing required columns. Found: {df.columns.tolist()}\")\n\n# --- Preprocess --- #\ndf = df.reset_index(drop=True)\ndf['Consciousness'] = df['Consciousness'].map(CONSCIOUSNESS_MAP)\ndf['Risk_Num'] = df['Risk_Level'].map(RISK_MAP)\ndf['On_Oxygen'] = pd.to_numeric(df['On_Oxygen'], errors='coerce').fillna(0).astype(int)\n\n# drop rows with missing features (small realistic datasets — alternative: impute)\nmissing_count = df[FEATURES].isna().sum().sum()\nif missing_count > 0:\n    print(f\"Dropping {df[FEATURES].isna().any(axis=1).sum()} rows with missing feature values.\")\n    df = df.loc[~df[FEATURES].isna().any(axis=1)].reset_index(drop=True)\n\nX = df[FEATURES].copy()\ny = df['Risk_Level'].copy()\ny_num = df['Risk_Num'].copy()\n\n# --- Train model --- #\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nscaler = StandardScaler()\n# We scale continuous vitals + keep categorical columns as-is for model input scaling only\nscaled_cols = CONTINUOUS_VITALS + ['O2_Scale','Consciousness','On_Oxygen']\nX_train_scaled = scaler.fit_transform(X_train[scaled_cols])\nX_test_scaled = scaler.transform(X_test[scaled_cols])\n\n# Train on raw values (forest can handle different scales) — but we keep scaler for clustering\nmodel = RandomForestClassifier(n_estimators=400, random_state=42)\nmodel.fit(X_train, y_train)\nprint(\"RandomForest trained. Test accuracy:\", round(model.score(X_test, y_test), 4))\n\n# --- SHAP (optional) --- #\ntry:\n    import shap\n    explainer = shap.TreeExplainer(model)\n    # compute sample shap values for a subset (speed)\n    sample = X_test.sample(min(200, len(X_test)), random_state=1)\n    shap_values = explainer.shap_values(sample)\n    SHAP_AVAILABLE = True\n    print(\"SHAP available.\")\nexcept Exception as e:\n    SHAP_AVAILABLE = False\n    print(\"SHAP not available:\", e)\n\n# --- Clustering (GMM) & Risk Drift --- #\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\ncluster_input = scaler.transform(X[scaled_cols])\nbest_k = 4; best_score = -1\nfor k in range(3,9):\n    g = GaussianMixture(n_components=k, random_state=42).fit(cluster_input)\n    labels = g.predict(cluster_input)\n    try:\n        s = silhouette_score(cluster_input, labels)\n    except Exception:\n        s = -1\n    if s > best_score:\n        best_score = s; best_k = k\n\ngmm = GaussianMixture(n_components=best_k, random_state=42).fit(cluster_input)\ndf['Cluster'] = gmm.predict(cluster_input)\ncentroids = gmm.means_\nprint(f\"GMM chosen clusters: {best_k} (silhouette {best_score:.3f})\")\n\ncluster_risk = df.groupby('Cluster')['Risk_Num'].mean().reindex(range(best_k)).fillna(0).values\ndist_matrix = np.linalg.norm(centroids[:, None] - centroids[None, :], axis=2)\nsimilarity = np.exp(-dist_matrix)\nrisk_drift_cluster = []\nfor a in range(best_k):\n    higher = cluster_risk > cluster_risk[a]\n    if higher.sum()==0:\n        risk_drift_cluster.append(0.0)\n    else:\n        risk_drift_cluster.append(float(similarity[a,higher].sum() / similarity[a].sum()))\nrisk_drift_cluster = np.array(risk_drift_cluster)\ndf['Risk_Drift'] = df['Cluster'].map(lambda c: float(risk_drift_cluster[c]))\ndf['Risk_Drift_Score'] = (100*df['Risk_Drift']).round(1)\n\n# --- Probabilistic deterioration (KDE-based) + Monte Carlo --- #\nfrom sklearn.neighbors import KernelDensity\n\nkde_models = {}\nfor vital in CONTINUOUS_VITALS:\n    vals = df[vital].values.reshape(-1,1)\n    low, high = SAFE_RANGES[vital]\n    residuals = []\n    for v in vals.flatten():\n        if v < low:\n            residuals.append((low - v))\n        elif v > high:\n            residuals.append((v - high))\n        else:\n            residuals.append(abs(np.random.normal(loc=0.2, scale=0.12)))\n    residuals = np.array(residuals).reshape(-1,1)\n    try:\n        kde = KernelDensity(kernel='gaussian', bandwidth=0.3).fit(np.log1p(residuals))\n        kde_models[vital] = ('kde', kde)\n    except Exception:\n        mu, sigma = float(residuals.mean()), float(residuals.std()) if residuals.std()>1e-6 else 0.5\n        kde_models[vital] = ('norm', (mu, sigma))\n\ndef sample_deterioration_single(base_row):\n    simulated = base_row.copy()\n    VITAL_BOUNDS = {\n        'Respiratory_Rate': (4, 60),\n        'Oxygen_Saturation': (40, 100),\n        'Systolic_BP': (50, 220),\n        'Heart_Rate': (30, 220),\n        'Temperature': (34.0, 42.0)\n    }\n    for vital in CONTINUOUS_VITALS:\n        low, high = SAFE_RANGES[vital]\n        orig = float(base_row[vital])\n        typ = kde_models[vital][0]\n        if typ == 'kde':\n            kde = kde_models[vital][1]\n            sampled = np.expm1(kde.sample(1)).flatten()[0]\n            sampled = max(0.01, float(sampled) * np.random.uniform(0.6,1.4))\n        else:\n            mu, sigma = kde_models[vital][1]\n            sampled = max(0.01, np.random.normal(mu, sigma))\n        if vital == \"Oxygen_Saturation\":\n            new_val = orig - sampled\n        elif vital == \"Respiratory_Rate\":\n            new_val = orig + sampled\n        elif vital == \"Heart_Rate\":\n            new_val = orig + sampled * np.random.uniform(0.9,1.3)\n        elif vital == \"Systolic_BP\":\n            if orig < low:\n                new_val = orig - sampled\n            elif orig > high:\n                new_val = orig + sampled * np.random.uniform(0.4,1.2)\n            else:\n                new_val = orig - sampled * np.random.uniform(0.2,0.9)\n        elif vital == \"Temperature\":\n            new_val = orig + sampled * np.random.uniform(0.2,0.6)\n        lo, hi = VITAL_BOUNDS[vital]\n        simulated[vital] = float(np.clip(new_val, lo, hi))\n    return simulated\n\ndef monte_carlo_future_prob(X_df, n_sims=200, required_label=\"High\"):\n    results = []\n    high_idx = list(model.classes_).index(required_label)\n    for idx, row in X_df.iterrows():\n        base_row = row.copy()\n        probs_high = []\n        for s in range(n_sims):\n            sim_row = sample_deterioration_single(base_row)\n            prob_high = model.predict_proba(pd.DataFrame([sim_row[FEATURES]]))[0][high_idx]\n            probs_high.append(prob_high)\n        probs = np.array(probs_high)\n        results.append({\n            'Patient_idx': idx,\n            'Patient_ID': df.loc[idx,'Patient_ID'],\n            'current_high_prob': model.predict_proba(pd.DataFrame([base_row[FEATURES]]))[0][high_idx],\n            'future_high_prob_mean': float(probs.mean()),\n            'future_high_prob_p95': float(np.percentile(probs,95)),\n            'future_high_prob_p05': float(np.percentile(probs,5)),\n            'future_high_prob_std': float(probs.std())\n        })\n    return pd.DataFrame(results)\n\n# run Monte Carlo on a sample (speed). Increase n_sims or sample size if time available.\nprint(\"Running Monte Carlo sample (this can take a while)...\")\nmc_sample = monte_carlo_future_prob(X.sample(min(200,len(X)), random_state=1), n_sims=200)\nprint(\"Monte Carlo sample done.\")\n\n# --- Adversarial minimal perturbation (Nelder-Mead) w/ fallback random search --- #\ntry:\n    from scipy.optimize import minimize\n    SCIPY_AVAILABLE = True\nexcept Exception:\n    SCIPY_AVAILABLE = False\n\nVITAL_BOUNDS = {\n    'Respiratory_Rate': (4, 60),\n    'Oxygen_Saturation': (40, 100),\n    'Systolic_BP': (50, 220),\n    'Heart_Rate': (30, 220),\n    'Temperature': (34.0, 42.0)\n}\n\ndef get_row_features_for_model(row):\n    return row[FEATURES]\n\ndef adversarial_minimal_change_for_row(idx, required_prob=0.5, target_label=\"High\", n_restarts=8, maxiter=300):\n    base_full = df.loc[idx].copy()\n    base = base_full[FEATURES].copy()\n    base_vals = np.array([base[v] for v in CONTINUOUS_VITALS], dtype=float)\n    target_index = list(model.classes_).index(target_label)\n    lower = np.array([VITAL_BOUNDS[v][0] for v in CONTINUOUS_VITALS])\n    upper = np.array([VITAL_BOUNDS[v][1] for v in CONTINUOUS_VITALS])\n\n    def build_input(x):\n        row_copy = base.copy()\n        for i,v in enumerate(CONTINUOUS_VITALS):\n            row_copy[v] = float(x[i])\n        return row_copy\n\n    def obj(x):\n        if np.any(x < lower) or np.any(x > upper):\n            return 1e6 + np.linalg.norm(x - base_vals)\n        prob = model.predict_proba(pd.DataFrame([build_input(x)]))[0][target_index]\n        dist = np.linalg.norm(x - base_vals)\n        penalty = 200.0 * max(0.0, required_prob - prob)\n        return dist + penalty\n\n    best = {'success': False, 'dist': np.inf, 'prob': None, 'x': None}\n    if SCIPY_AVAILABLE:\n        for r in range(n_restarts):\n            x0 = np.clip(base_vals + np.random.normal(scale=0.5, size=len(base_vals)), lower, upper)\n            res = minimize(obj, x0, method='Nelder-Mead', options={'maxiter':maxiter, 'xatol':1e-3, 'fatol':1e-3, 'disp': False})\n            x_opt = np.clip(res.x, lower, upper)\n            prob = model.predict_proba(pd.DataFrame([build_input(x_opt)]))[0][target_index]\n            dist = float(np.linalg.norm(x_opt - base_vals))\n            if prob >= required_prob and dist < best['dist']:\n                best.update({'success': True, 'dist': dist, 'prob': float(prob), 'x': x_opt.copy()})\n    else:\n        # Random-search fallback\n        for t in range(3000):\n            x_try = np.clip(base_vals + np.random.normal(scale=1.0, size=len(base_vals)), lower, upper)\n            prob = model.predict_proba(pd.DataFrame([build_input(x_try)]))[0][target_index]\n            dist = float(np.linalg.norm(x_try - base_vals))\n            if prob >= required_prob and dist < best['dist']:\n                best.update({'success': True, 'dist': dist, 'prob': float(prob), 'x': x_try.copy()})\n\n    # if we didn't find successful flip, provide best prob found:\n    if not best['success']:\n        best_prob = -1; best_x = None\n        for t in range(800):\n            x_try = np.clip(base_vals + np.random.normal(scale=1.2, size=len(base_vals)), lower, upper)\n            prob = model.predict_proba(pd.DataFrame([build_input(x_try)]))[0][target_index]\n            if prob > best_prob:\n                best_prob = float(prob); best_x = x_try.copy()\n        best.update({'success': False, 'dist': float(np.linalg.norm(best_x - base_vals)), 'prob': float(best_prob), 'x': best_x})\n\n    deltas = {v: float(best['x'][i] - base_vals[i]) for i,v in enumerate(CONTINUOUS_VITALS)} if best['x'] is not None else None\n    new_vals = {v: float(best['x'][i]) for i,v in enumerate(CONTINUOUS_VITALS)} if best['x'] is not None else None\n\n    return {\n        'Patient_idx': idx, 'Patient_ID': df.loc[idx,'Patient_ID'],\n        'success': best['success'], 'distance': best['dist'],\n        'prob_high': best['prob'], 'deltas': deltas, 'new_values': new_vals\n    }\n\n# Run adversarial search on a sample (speed)\nsample_idxs = list(X.sample(min(60,len(X)), random_state=2).index)\nadv_results = [adversarial_minimal_change_for_row(i, required_prob=0.5, n_restarts=8) for i in sample_idxs]\nadv_df = pd.DataFrame(adv_results)\nadv_df_sorted = adv_df.sort_values(by=['success','distance','prob_high'], ascending=[False, True, False])\n\n# Combine Monte Carlo and adversarial results for the same sampled patients\ncombined = pd.merge(\n    adv_df, \n    mc_sample, \n    on=['Patient_idx', 'Patient_ID'],  # Join on both index and Patient_ID\n    how='left'\n)\ncombined['Risk_Drift_Score'] = combined['Patient_idx'].map(lambda i: df.loc[i,'Risk_Drift_Score'])\ncombined['Current_Risk'] = combined['Patient_idx'].map(lambda i: df.loc[i,'Risk_Level'])\n\n# Save outputs\ncombined.to_csv(\"combined_fragility_and_mc_summary.csv\", index=False)\nadv_df_sorted.to_csv(\"adversarial_summary_sample.csv\", index=False)\nmc_sample.to_csv(\"monte_carlo_summary_sample.csv\", index=False)\n\nprint(\"Saved files: combined_fragility_and_mc_summary.csv, adversarial_summary_sample.csv, monte_carlo_summary_sample.csv\")\nprint(\"Top fragile patients (adversarial flip found with small distance):\")\nprint(adv_df_sorted[adv_df_sorted['success']].sort_values('distance').head(10))\n\n# If in a Jupyter environment, display small samples\ntry:\n    from IPython.display import display\n    display(adv_df_sorted.head(20))\n    display(mc_sample.head(20))\n    display(combined.head(20))\nexcept Exception:\n    pass\n\n# End of pipeline",
      "block_group": "5c3b1e9353cd4e4f88c4983750bf5c93",
      "execution_count": 1,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "b623e53d",
        "execution_start": 1763248858409,
        "execution_millis": 44,
        "execution_context_id": "e448880f-9f97-446f-944a-f74151a5847a",
        "cell_id": "6f193abfd09a421e91db8d33857d88f5",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "block_group": "00d6570a8cac407d8ec4c1c7cb94c839",
      "execution_count": 1,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=bdee0b07-dd93-46b8-8522-cce134092e71' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_persisted_session": {
      "createdAt": "2025-11-15T23:26:03.611Z"
    },
    "deepnote_notebook_id": "4bf6aedcba0940fc8bced221092e8af8"
  }
}